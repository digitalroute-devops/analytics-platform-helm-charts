servicesDomain: ""

ingressClass: nginx

authProxy:
  name: auth-proxy
  image: quay.io/mojanalytics/auth-proxy
  tag: v0.1.6
  imagePullPolicy: IfNotPresent
  containerPort: 3000
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 25m
      memory: 64Mi
  cookie_maxage: 36000 #10 hours
  cookie_secret: "tony-montana"

apps:
  - name: alertmanager
    port: 9093
  - name: prometheus
    port: 9090

serviceMonitors:
  - labelSelector: nginx-ingress
    targetNamespace: default
    targetPort: metrics
 # - labelSelector: kubernetes-dashboard
 #   targetNamespace: kube-system
 #   targetPath: /metrics
 #   targetPort: 8443
 #   scrapeInterval: 20s



prometheusOperator:
  crdApiGroup: monitoring.coreos.com
  chartName: prometheus-operator

prometheusRules:
    - name: analytics.rules
      rules:
      - alert: InstanceHighMemoryUsage
        annotations:
          description: '{{ $value }}% of cluster memory is in use'
          summary: High Memory Load
         # Alert if disk utilisation reaches 90%
        expr: (sum(node_memory_MemTotal_bytes) - sum(node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes) ) / sum(node_memory_MemTotal_bytes) * 100 > 90
        for: 10m
        labels:
          severity: warning

      - alert: PreditciveHostDiskSpace
        annotations:
          description: 'Based on recent sampling, the disk is likely to will fill on volume
             {{ $labels.mountpoint }} within the next 5 hours for instace: {{ $labels.instance_id
             }} tagged as: {{ $labels.instance_name_tag }}'
          summary: Predictive Disk Space Utilisation Alert
        expr: predict_linear(node_filesystem_free_bytes{mountpoint="/"}[4h], 5 * 3600) < 0
        for: 10h
        labels:
          severity: warning

      - alert: HighNumberOfUserHTTPErrors
        annotations:
          description: 'Ingress: {{ $labels.ingress }} has returned {{ $value }} 503 errors in the last 10 minutes'
          summary: User getting a high number of 503 errors
        expr: increase(nginx_ingress_controller_requests{status=~"503", exported_namespace=~"user-.*"}[10m]) > 5
        for: 1m
        label:
          severity: warning

      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Reloading Prometheus' configuration has failed for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Reloading Promehteus' configuration failed

      - alert: PrometheusNotificationQueueRunningFull
        expr: predict_linear(prometheus_notifications_queue_length[5m], 60 * 30) > prometheus_notifications_queue_capacity
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Prometheus' alert notification queue is running full for {{ $labels.namespace }}/{{
            $labels.pod }}
          summary: Prometheus' alert notification queue is running full

      - alert: PrometheusErrorSendingAlerts
        expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
          > 0.01
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Errors while sending alerts from Prometheus {{ $labels.namespace }}/{{
            $labels.pod }} to Alertmanager {{ $labels.Alertmanager }}
          summary: Errors while sending alert from Prometheus

      - alert: PrometheusErrorSendingAlerts
        expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
          > 0.03
        for: 10m
        labels:
          severity: critical
        annotations:
          description: Errors while sending alerts from Prometheus {{ $labels.namespace }}/{{
            $labels.pod }} to Alertmanager {{ $labels.Alertmanager }}
          summary: Errors while sending alerts from Prometheus

      - alert: PrometheusNotConnectedToAlertmanagers
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not connected
            to any Alertmanagers
          summary: Prometheus is not connected to any Alertmanagers

      - alert: PrometheusTSDBReloadsFailing
        expr: increase(prometheus_tsdb_reloads_failures_total[2h]) > 0
        for: 12h
        labels:
          severity: warning
        annotations:
          description: '{{ $labels.job }} at {{ $labels.instance }} had {{ $value | humanize }}
        reload failures over the last four hours.'
          summary: Prometheus has issues reloading data blocks from disk

      - alert: PrometheusTSDBCompactionsFailing
        expr: increase(prometheus_tsdb_compactions_failed_total[2h]) > 0
        for: 12h
        labels:
          severity: warning
        annotations:
          description: '{{ $labels.job }} at {{ $labels.instance }} had {{ $value | humanize }}
        compaction failures over the last four hours.'
          summary: Prometheus has issues compacting sample blocks

      - alert: PrometheusTSDBWALCorruptions
        expr: tsdb_wal_corruptions_total > 0
        for: 4h
        labels:
          severity: warning
        annotations:
          description: '{{ $labels.job }} at {{ $labels.instance }} has a corrupted write-ahead
        log (WAL).'
          summary: Prometheus write-ahead log is corrupted

      - alert: PrometheusNotIngestingSamples
        expr: rate(prometheus_tsdb_head_samples_appended_total[5m]) <= 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not ingesting samples.'
          summary: "Prometheus isn't ingesting samples"

      - alert: PrometheusTargetScrapesDuplicate
        expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: '{{ $labels.namespace }}/{{ $labels.pod }} has many samples rejected due to duplicate timestamps but different values'
          summary: Prometheus has many samples rejected
